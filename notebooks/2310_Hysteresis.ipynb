{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports\n",
    "\n",
    "Import python libraries as well as the self written FERMI library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from os.path import join, split\n",
    "from getpass import getuser\n",
    "from glob import glob\n",
    "from time import strftime\n",
    "from tqdm.auto import tqdm\n",
    "from importlib import reload\n",
    "from copy import deepcopy\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "# Images\n",
    "import imageio\n",
    "from imageio import imread\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import NonUniformImage\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# pyFAI\n",
    "import pyFAI\n",
    "from pyFAI.azimuthalIntegrator import AzimuthalIntegrator\n",
    "from pyFAI.detectors import Detector\n",
    "\n",
    "# Self-written libraries\n",
    "import process_FERMI as pf\n",
    "from process_FERMI import interactive\n",
    "from proces_FERMI.interactive import cimshow\n",
    "\n",
    "plt.rcParams[\"figure.constrained_layout.use\"] = True  # replaces plt.tight_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# interactive plotting\n",
    "import ipywidgets\n",
    "%matplotlib widget\n",
    "plt.rcParams[\"figure.constrained_layout.use\"] = True\n",
    "\n",
    "# Auto formatting of cells\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder):\n",
    "    \"\"\"\n",
    "    Creates input folder if it does not exist yet\n",
    "    \"\"\"\n",
    "\n",
    "    if not (os.path.exists(folder)):\n",
    "        print(\"Creating folder \" + folder)\n",
    "        os.makedirs(folder)\n",
    "    return folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hysteresis(exp, dark, mask, magnet_string=\"magnet_mean\"):\n",
    "    print(\"Loading images and summing the scattering.\")\n",
    "    total_intensities = np.empty(len(exp))\n",
    "    for i in tqdm(range(len(exp))):\n",
    "        imi, _ = pf.loadh5(\n",
    "            exp.iloc[i][\"filename\"], extra_keys=[\"alignz\", \"PAM/FQPDSum\"]\n",
    "        )\n",
    "        total_intensities[i] = np.sum(\n",
    "            (imi.astype(float) - dark.astype(float)) * mask.astype(float)\n",
    "        )\n",
    "\n",
    "    print(\"Averaging the images at the same magnetic field.\")\n",
    "    total_intensity = []\n",
    "    magnet = np.unique(exp[magnet_string])\n",
    "    for m in tqdm(magnet):\n",
    "        idx = np.where(exp[magnet_string] == m)[0]\n",
    "        total_intensity.append(np.mean(total_intensities[idx]))\n",
    "\n",
    "    return np.array(total_intensity), magnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(fname, extra_keys, magnet_function, dataname=None, darkname=None):\n",
    "    if dataname is not None:\n",
    "        fname = fnm\n",
    "\n",
    "    exp = pf.get_exp_dataframe(fname, keys=extra_keys)\n",
    "    for k in [\"xgm_UH\", \"xgm_SH\", \"diode_sum\"]:\n",
    "        exp[k + \"_sum\"] = exp[k].apply(np.sum)\n",
    "\n",
    "    exp[\"diode_sum_mean\"] = exp.diode_sum.apply(np.mean)\n",
    "    exp[\"diode_sum_std\"] = exp.diode_sum.apply(np.std)\n",
    "    exp[\"IR_mean\"] = exp.IR.apply(np.mean)\n",
    "    exp[\"IR_std\"] = exp.IR.apply(np.std)\n",
    "    exp[\"magnet_mean\"] = exp.magnet.apply(magnet_function)\n",
    "    exp[\"magnet_mean\"] = exp.magnet_mean.apply(np.round, args=(3,))\n",
    "    exp[\"bunchid\"] = exp.bunches.apply(lambda l: l[-1])\n",
    "\n",
    "    if darkname is not None:\n",
    "        exp_bg = pf.get_exp_dataframe(darkname, keys=extra_keys)\n",
    "        exp_bg[\"bunchid\"] = exp.bunches.apply(lambda l: l[-1])\n",
    "        exp_bg = exp_bg.sort_values(\"time\")\n",
    "\n",
    "        darkfolder = datafolder + \"_BG\"\n",
    "        darks = []\n",
    "        dflist = glob(darkfolder + \"/**/*h5\", recursive=True)\n",
    "        print(f\"found {len(dflist)} dark files\")\n",
    "        for f in dflist:\n",
    "            _dark = pf.loadh5(f)[0]\n",
    "            darks.append(_dark)\n",
    "        dark = np.mean(darks, axis=0).astype(float)\n",
    "\n",
    "        return (exp, dark)\n",
    "    return dark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_exp_OF(datafolder, keys=None, sort=False):\n",
    "    # Loading experiment data\n",
    "    extension = \"_OF\"\n",
    "    exp = pf.get_exp_dataframe(datafolder + extension, keys=keys)\n",
    "    for k in [\"xgm_UH\", \"xgm_SH\", \"diode_sum\"]:\n",
    "        exp[k + \"_sum\"] = exp[k].apply(np.sum)\n",
    "\n",
    "    exp[\"diode_sum_mean\"] = exp.diode_sum.apply(np.mean)\n",
    "    exp[\"diode_sum_std\"] = exp.diode_sum.apply(np.std)\n",
    "    exp[\"IR_mean\"] = exp.IR.apply(np.mean)\n",
    "    exp[\"IR_std\"] = exp.IR.apply(np.std)\n",
    "    exp[\"magnet_mean\"] = exp.magnet.apply(np.mean)\n",
    "    exp[\"magnet_mean\"] = exp.magnet_mean.apply(np.round, args=(3,))\n",
    "    exp[\"bunchid\"] = exp.bunches.apply(lambda l: l[-1])\n",
    "\n",
    "    if sort is True:\n",
    "        exp = exp.sort_values(scan_axis)\n",
    "\n",
    "    # Loading image data\n",
    "    exp[\"images\"] = [\n",
    "        pf.loadh5(fname, extra_keys=[\"alignz\", \"PAM/FQPDSum\"])[0]\n",
    "        for fname in exp[\"filename\"]\n",
    "    ]\n",
    "\n",
    "    return exp\n",
    "\n",
    "\n",
    "def preprocess_exp_BG(datafolder, keys=None):\n",
    "    # Loading background exp dataframe\n",
    "    extension = \"_BG\"\n",
    "    exp_bg = pf.get_exp_dataframe(datafolder + extension, keys=keys)\n",
    "    exp_bg[\"bunchid\"] = exp.bunches.apply(lambda l: l[-1])\n",
    "    exp_bg = exp_bg.sort_values(\"time\")\n",
    "\n",
    "    # Loading background data\n",
    "    exp_bg[\"images\"] = [\n",
    "        pf.loadh5(fname, extra_keys=[\"alignz\", \"PAM/FQPDSum\"])[0]\n",
    "        for fname in exp_bg[\"filename\"]\n",
    "    ]\n",
    "\n",
    "    return exp_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    \"\"\"\n",
    "    Normalize data to [-1,1] range for hysteresis\n",
    "    \"\"\"\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw circle mask\n",
    "def circle_mask(shape, center, radius, sigma=\"none\"):\n",
    "    \"\"\"\n",
    "    Draws circle mask with option to apply gaussian filter for smoothing\n",
    "\n",
    "    Parameter\n",
    "    =========\n",
    "    shape : int tuple\n",
    "        shape/dimension of output array\n",
    "    center : int tuple\n",
    "        center coordinates (ycenter,xcenter)\n",
    "    radius : scalar\n",
    "        radius of mask in px. Care: diameter is always (2*radius+1) px\n",
    "    sigma : scalar\n",
    "        std of gaussian filter\n",
    "\n",
    "    Output\n",
    "    ======\n",
    "    mask: array\n",
    "        binary mask, or smoothed binary mask\n",
    "    ======\n",
    "    author: ck 2022\n",
    "    \"\"\"\n",
    "\n",
    "    # setup array\n",
    "    x = np.linspace(0, shape[1] - 1, shape[1])\n",
    "    y = np.linspace(0, shape[0] - 1, shape[0])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    # define circle\n",
    "    mask = np.sqrt(((X - center[1]) ** 2 + (Y - center[0]) ** 2)) <= (radius)\n",
    "    mask = mask.astype(float)\n",
    "\n",
    "    # smooth aperture\n",
    "    if sigma != \"none\":\n",
    "        mask = scipy.ndimage.filters.gaussian_filter(mask, sigma)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_single_polygon_mask(shape, coordinates):\n",
    "    \"\"\"\n",
    "    Creates a polygon mask from coordinates of corner points\n",
    "\n",
    "    Parameter\n",
    "    =========\n",
    "    shape : int tuple\n",
    "        shape/dimension of output array\n",
    "    coordinates: nested list\n",
    "        coordinates of polygon corner points [[yc_1,xc_1],[yc_2,xc_2],...]\n",
    "\n",
    "\n",
    "    Output\n",
    "    ======\n",
    "    mask: array\n",
    "        binary mask where filled polygon is \"1\"\n",
    "    ======\n",
    "    author: ck 2023\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n",
    "    x, y = x.flatten(), y.flatten()\n",
    "\n",
    "    points = np.vstack((x, y)).T\n",
    "\n",
    "    path = Path(coordinates)\n",
    "    mask = path.contains_points(points)\n",
    "    mask = mask.reshape(shape)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_polygon_mask(shape, coordinates):\n",
    "    \"\"\"\n",
    "    Creates multiple polygon masks from set of coordinates of corner points\n",
    "\n",
    "    Parameter\n",
    "    =========\n",
    "    shape : int tuple\n",
    "        shape/dimension of output array\n",
    "    coordinates: nested list\n",
    "        coordinates of polygon corner points for multiple polygons\n",
    "        [[[yc_1,xc_1],[yc_2,xc_2],...],[[yc_1,xc_1],[yc_2,xc_2],...]]\n",
    "\n",
    "    Output\n",
    "    ======\n",
    "    mask: array\n",
    "        binary mask where filled polygons are \"1\"\n",
    "    ======\n",
    "    author: ck 2023\n",
    "    \"\"\"\n",
    "\n",
    "    if len(coordinates) == 1:\n",
    "        mask = create_single_polygon_mask(shape, coordinates[0])\n",
    "    elif len(coordinates) > 1:\n",
    "        mask = np.zeros(shape)\n",
    "        for coord in coordinates:\n",
    "            mask = mask + create_single_polygon_mask(shape, coord)\n",
    "            mask[mask > 1] = 1\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def load_poly_masks(polygon_name_list, shape):\n",
    "    \"\"\"\n",
    "    Loads set of polygon masks based on stored coordinates\n",
    "\n",
    "    Parameter\n",
    "    =========\n",
    "    polygon_name_list : list\n",
    "        shape/dimension of output array\n",
    "\n",
    "\n",
    "    Output\n",
    "    ======\n",
    "    mask: array\n",
    "        binary mask where filled polygons are \"1\"\n",
    "    ======\n",
    "    author: ck 2023\n",
    "    \"\"\"\n",
    "\n",
    "    mask = []\n",
    "    for polygon_name in polygon_name_list:\n",
    "        coord = load_poly_coordinates(polygon_name)\n",
    "        mask.append(create_polygon_mask(shape, [coord]).astype(float))\n",
    "\n",
    "    mask = np.array(mask)\n",
    "    mask = np.sum(mask, axis=0)\n",
    "    mask[mask > 1] = 1\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic folders\n",
    "BASEFOLDER = r\"/net/online4diproi/store/\"\n",
    "PROPOSAL = \"20224053\"\n",
    "basefolder = join(BASEFOLDER, PROPOSAL)\n",
    "USER = getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict with most basic experimental parameter\n",
    "experimental_setup = {\n",
    "    \"ccd_dist\": 0.103,  # ccd to sample distance\n",
    "    \"px_size\": 11e-6,  # pixel_size of camera\n",
    "    \"binning\": 1,  # Camera binning\n",
    "}\n",
    "\n",
    "# Setup for azimuthal integrator\n",
    "detector = Detector(\n",
    "    experimental_setup[\"binning\"] * experimental_setup[\"px_size\"],\n",
    "    experimental_setup[\"binning\"] * experimental_setup[\"px_size\"],\n",
    ")\n",
    "\n",
    "# General saving folder\n",
    "folder_general = create_folder(join(basefolder, \"results\", \"processed\"))\n",
    "print(\"Output Folder: %s\" % folder_general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define for loading\n",
    "sample = \"S2205ref\"\n",
    "membrane = \"I0\"\n",
    "scan_id = 11\n",
    "scan = f\"%s_HystScan_Inverted_%03d\" % (membrane, scan_id)\n",
    "scan_axis = \"magnet_mean\"\n",
    "extension = \"_OF\"\n",
    "\n",
    "# Folder for loading\n",
    "samplefolder = join(sample, scan)\n",
    "datafolder = join(basefolder, samplefolder)\n",
    "extra_keys = {\n",
    "    \"diode_sum\": \"PAM/FQPDSum\",\n",
    "    \"IR\": \"Laser/Energy1\",\n",
    "    \"magnet\": \"DPI/CoilCurrent\",\n",
    "    \"bunches\": \"bunches\",\n",
    "    \"time\": \"\",\n",
    "}\n",
    "\n",
    "# Create savefolder\n",
    "fsave = create_folder(join(folder_general, sample, membrane))\n",
    "\n",
    "# Loading experiment data\n",
    "exp = pf.get_exp_dataframe(datafolder + extension, keys=extra_keys)\n",
    "for k in [\"xgm_UH\", \"xgm_SH\", \"diode_sum\"]:\n",
    "    exp[k + \"_sum\"] = exp[k].apply(np.sum)\n",
    "\n",
    "exp[\"diode_sum_mean\"] = exp.diode_sum.apply(np.mean)\n",
    "exp[\"diode_sum_std\"] = exp.diode_sum.apply(np.std)\n",
    "exp[\"IR_mean\"] = exp.IR.apply(np.mean)\n",
    "exp[\"IR_std\"] = exp.IR.apply(np.std)\n",
    "exp[\"magnet_mean\"] = exp.magnet.apply(np.mean)\n",
    "exp[\"magnet_mean\"] = exp.magnet_mean.apply(np.round, args=(3,))\n",
    "exp[\"bunchid\"] = exp.bunches.apply(lambda l: l[-1])\n",
    "# exp = exp.sort_values(scan_axis)\n",
    "\n",
    "# Add wavelength\n",
    "experimental_setup[\"lambda\"] = exp[\"wavelength\"][0] * 1e-9\n",
    "\n",
    "# Loading image data\n",
    "exp[\"images\"] = [\n",
    "    pf.loadh5(fname, extra_keys=[\"alignz\", \"PAM/FQPDSum\"])[0]\n",
    "    for fname in exp[\"filename\"]\n",
    "]\n",
    "print(\"Data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What did you scan?\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(exp)), exp[scan_axis], \"-o\")\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(scan_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dark images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dark image\n",
    "extension = \"_BG\"\n",
    "exp_bg = pf.get_exp_dataframe(datafolder + extension, keys=extra_keys)\n",
    "exp_bg[\"bunchid\"] = exp.bunches.apply(lambda l: l[-1])\n",
    "exp_bg = exp_bg.sort_values(\"time\")\n",
    "\n",
    "# Loading image data\n",
    "exp_bg[\"images\"] = [\n",
    "    pf.loadh5(fname, extra_keys=[\"alignz\", \"PAM/FQPDSum\"])[0]\n",
    "    for fname in exp_bg[\"filename\"]\n",
    "]\n",
    "\n",
    "dark = np.mean(np.stack(exp_bg[\"images\"]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot images\n",
    "fig, ax = cimshow(dark)\n",
    "fig.set_size_inches(6, 6)\n",
    "ax.set_title(\"Dark Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract dark image and normalize with I0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incident intensity\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(exp)), exp[\"diode_sum_mean\"])\n",
    "ax.set_xlabel(\"Image Index\")\n",
    "ax.set_ylabel(\"Incident Intensity (a.u.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute\n",
    "images = np.stack(exp.images) - dark\n",
    "images = images / np.broadcast_to(np.array(exp[\"diode_sum_mean\"]), images.T.shape).T\n",
    "im_mean = np.mean(images, axis=0)\n",
    "\n",
    "# Plot images\n",
    "fig, ax = cimshow(images)\n",
    "fig.set_size_inches(6, 6)\n",
    "ax.set_title(\"Images of scan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw beamstop mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_mask = interactive.draw_polygon_mask(im_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take poly coordinates and mask from widget\n",
    "p_coord = poly_mask.coordinates\n",
    "mask = poly_mask.full_mask.astype(int)\n",
    "\n",
    "cimshow(mask)\n",
    "\n",
    "print(\"Mask coordinates: %s\" % p_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_poly_coordinates(polyname):\n",
    "    if polyname == \"bs_cross\":\n",
    "        coord = [\n",
    "            (946.9258289571189, 901.9078818510652),\n",
    "            (-13.52166457167948, 915.628560330048),\n",
    "            (-8.033393180086364, 1107.7180590358075),\n",
    "            (944.1816932613224, 1099.4856519484179),\n",
    "            (946.9258289571189, 1988.5856173865054),\n",
    "            (945.4332858438797, 2042.780748713184),\n",
    "            (1134.4167167610876, 2056.6765892218023),\n",
    "            (1145.5333891679823, 1092.305257923697),\n",
    "            (2007.0755007023129, 1106.2010984323151),\n",
    "            (2067.2340362524137, 1100.0546685633194),\n",
    "            (2067.2340362524137, 910.7093055533562),\n",
    "            (1147.9485781605636, 902.4768984659665),\n",
    "            (1153.4368495521567, -38.761645192255855),\n",
    "            (955.8590794548038, -22.296831017476507),\n",
    "        ]\n",
    "\n",
    "    return coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which drawn masks do you want to load?\n",
    "polygon_names = [\"bs_cross\"]\n",
    "mask = load_poly_masks(polygon_names, images[0].shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "mi, ma = np.percentile(im_mean, [1, 99])\n",
    "ax[0].imshow(im_mean * (1 - mask), vmin=mi, vmax=ma)\n",
    "ax[0].set_title(\"(1-mask)\")\n",
    "ax[1].imshow(im_mean * mask, vmin=mi, vmax=ma)\n",
    "ax[1].set_title(\"mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic widget to find center\n",
    "\n",
    "Try to **align** the circles to the **center of the scattering pattern**. Care! Position of beamstop might be misleading and not represent the actual center of the hologram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set center position via widget\n",
    "ic = interactive.InteractiveCenter(im_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get center positions\n",
    "center = [ic.c0, ic.c1]\n",
    "print(f\"Center:\", center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azimuthal integrator widget for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup azimuthal integrator for virtual geometry\n",
    "ai = interactive.AzimuthalIntegrator(\n",
    "    dist=experimental_setup[\"ccd_dist\"],\n",
    "    detector=detector,\n",
    "    wavelength=experimental_setup[\"lambda\"],\n",
    "    poni1=center[0]\n",
    "    * experimental_setup[\"px_size\"]\n",
    "    * experimental_setup[\"binning\"],  # y (vertical)\n",
    "    poni2=center[1]\n",
    "    * experimental_setup[\"px_size\"]\n",
    "    * experimental_setup[\"binning\"],  # x (horizontal)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting to find  relevant q range\n",
    "I_t, q_t, phi_t = ai.integrate2d(\n",
    "    im_mean,\n",
    "    200,\n",
    "    radial_range=(0, 0.05),\n",
    "    unit=\"q_nm^-1\",\n",
    "    correctSolidAngle=False,\n",
    "    dummy=np.nan,\n",
    "    mask=mask,\n",
    ")\n",
    "az2d = xr.DataArray(I_t, dims=(\"phi\", \"q\"), coords={\"q\": q_t, \"phi\": phi_t})\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "mi, ma = np.nanpercentile(I_t, [1, 99])\n",
    "az2d.plot.imshow(ax=ax, vmin=mi, vmax=ma)\n",
    "plt.title(f\"Azimuthal integration\")\n",
    "\n",
    "# Vertical lines\n",
    "# q_lines = [0.025, 0.05]\n",
    "# for qt in q_lines:\n",
    "#    ax.axvline(qt, ymin=0, ymax=180, c=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic = interactive.AzimuthalIntegrationCenter(\n",
    "    # np.log10(images[36] - np.min(images[36]) + 1),\n",
    "    images[36],\n",
    "    ai,\n",
    "    c0=center[0],\n",
    "    c1=center[1],\n",
    "    mask=mask,\n",
    "    im_data_range=[1, 90],\n",
    "    radial_range=(0.002, 0.03),\n",
    "    qlines=[40, 60],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get center positions\n",
    "center = [aic.c0, aic.c1]\n",
    "print(f\"Center:\", center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Azimuthal Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup final azimuthal integrator for virtual geometry\n",
    "ai = interactive.AzimuthalIntegrator(\n",
    "    dist=experimental_setup[\"ccd_dist\"],\n",
    "    detector=detector,\n",
    "    wavelength=experimental_setup[\"lambda\"],\n",
    "    poni1=center[0]\n",
    "    * experimental_setup[\"px_size\"]\n",
    "    * experimental_setup[\"binning\"],  # y (vertical)\n",
    "    poni2=center[1]\n",
    "    * experimental_setup[\"px_size\"]\n",
    "    * experimental_setup[\"binning\"],  # x (horizontal)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 2d Azimuthal integration of all images and add to xarray\n",
    "list_i2d = []\n",
    "for im in tqdm(images):\n",
    "    i2d, q, chi = ai.integrate2d(im, 500, 90, dummy=np.nan, mask=mask)\n",
    "    list_i2d.append(i2d)\n",
    "\n",
    "# Setup xarray\n",
    "data = xr.Dataset()\n",
    "data[\"images\"] = xr.DataArray(images, dims=[\"index\", \"y\", \"x\"])\n",
    "data[scan_axis] = xr.DataArray(exp[scan_axis], dims=[\"index\"])\n",
    "data[\"q\"] = q\n",
    "data[\"chi\"] = chi\n",
    "data[\"i2d\"] = xr.DataArray(list_i2d, dims=[\"index\", \"chi\", \"q\"])\n",
    "data = data.assign_attrs({\"center\": center})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select relevant chi-range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2d and 1d azimuthal integration to estimate the relevant chi and q range\n",
    "# which image to show?\n",
    "idx = 26\n",
    "\n",
    "# Select chi-range\n",
    "# sel = (data.chi > -40) * (data.chi < 125) + (data.chi < -76) * (data.chi > 100)\n",
    "# data[\"i1d\"] = data.i2d.where(sel, drop=True).mean(\"chi\")\n",
    "data[\"i1d\"] = data.i2d.mean(\"chi\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(\n",
    "    2,\n",
    "    1,\n",
    "    figsize=(8, 8),\n",
    "    sharex=True,\n",
    ")\n",
    "mi, ma = np.nanpercentile(I_t, [0.1, 90])\n",
    "data[\"i2d\"][idx].plot.imshow(ax=ax[0], vmin=mi, vmax=ma)\n",
    "ax[0].set_title(f\"2d Azimuthal integration\")\n",
    "ax[0].grid()\n",
    "\n",
    "# Plot 1d azimuthal integration to estimate the relevant q-range\n",
    "ax[1].plot(data.q, data.i1d[idx])\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].set_title(\"1d Azimuthal Integration\")\n",
    "ax[1].grid()\n",
    "ax[1].set_ylabel(\"Integrated intensity\")\n",
    "ax[1].set_xlabel(\"q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select relevant q-range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant q-range for averaging\n",
    "q0, q1 = 0.005, 0.04\n",
    "binning = False\n",
    "bins = []\n",
    "\n",
    "# Get SAXS from q-range\n",
    "sel = (data.q > q0) * (data.q < q1)\n",
    "data[\"saxs\"] = data.i1d.where(sel, drop=True).mean(\"q\")\n",
    "\n",
    "# Averaging of same scan axis values or binning\n",
    "if binning is True:\n",
    "    # Execute binning\n",
    "    data_bin = data.groupby_bins(scan_axis, bins).mean()\n",
    "\n",
    "    # Rename binned values, drop intervals as those cannot be save in h5\n",
    "    bin_scan_axis = scan_axis + \"_bins\"\n",
    "    data_bin = data_bin.swap_dims({bin_scan_axis: scan_axis})\n",
    "    data_bin = data_bin.drop(bin_scan_axis)\n",
    "else:\n",
    "    _, count = np.unique(data[scan_axis].values, return_counts=True)\n",
    "    if np.any(count > 1):\n",
    "        data_bin = data.groupby(scan_axis).mean()\n",
    "\n",
    "# Add scan identifier\n",
    "data_bin[\"scan\"] = scan\n",
    "\n",
    "# Add AI mask\n",
    "data_bin[\"mask\"] = xr.DataArray(mask, dims=[\"y\", \"x\"])\n",
    "\n",
    "# Direction of \"time\"\n",
    "if np.sum(data[scan_axis][1:].values - data[scan_axis][0:-1].values) >= 0:\n",
    "    data_bin[\"order\"] = 1\n",
    "elif np.sum(data[scan_axis][1:].values - data[scan_axis][0:-1].values) < 0:\n",
    "    data_bin[\"order\"] = -1\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    data_bin[scan_axis].values,\n",
    "    data_bin[\"saxs\"].values,\n",
    "    \"o-\",\n",
    "    label=\"Azimuthal Integration\",\n",
    ")\n",
    "# ax.plot(\n",
    "#    data_bin[scan_axis].values,\n",
    "#    np.mean(data_bin[\"images\"].values * (1 - mask), axis=(1, 2)),\n",
    "#    label=\"Simple Mean\",\n",
    "# )\n",
    "\n",
    "ax.set_xlabel(scan_axis)\n",
    "ax.set_ylabel(\"Integrated SAXS\")\n",
    "ax.set_title(\"Scan: %s\" % scan)\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "# Save fig\n",
    "fname = join(fsave, \"Hysteresis_%s_%s.png\" % (scan, USER))\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hysteresis Plot\n",
    "\n",
    "## Select roi for plotting\n",
    "\n",
    "How to use:\n",
    "1. Zoom into the image and adjust your FOV until you are satisfied.\n",
    "2. Save the axes coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = cimshow(data_bin[\"images\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes start and end of x and y axis\n",
    "x1, x2 = ax.get_xlim()\n",
    "y2, y1 = ax.get_ylim()\n",
    "roi = np.array([int(y1), int(y2), int(x1), int(x2)])\n",
    "roi_s = np.s_[roi[0] : roi[1], roi[2] : roi[3]]\n",
    "print(f\"Roi:\", roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find max and min considering all images\n",
    "allmin, allmax = np.nanpercentile(data_bin[\"i2d\"].values, [3, 97])\n",
    "print(\"Min: %d Max: %d\" % (allmin, allmax))\n",
    "\n",
    "# Create folder for gif single frames\n",
    "folder_gif = create_folder(join(fsave, \"Scan_%s\" % scan))\n",
    "\n",
    "im_fnames = []\n",
    "for i in tqdm(range(len(data_bin[scan_axis].values))):\n",
    "    # Plot for averaged image\n",
    "    fig = plt.figure(figsize=(6.5, 10))\n",
    "    gs1 = gridspec.GridSpec(\n",
    "        2,\n",
    "        1,\n",
    "        figure=fig,\n",
    "        left=0.2,\n",
    "        bottom=0.05,\n",
    "        right=0.975,\n",
    "        top=1.1,\n",
    "        wspace=0,\n",
    "        hspace=0,\n",
    "        height_ratios=[3, 1],\n",
    "    )\n",
    "\n",
    "    ax0 = fig.add_subplot(gs1[0])\n",
    "    tmp = data_bin[\"images\"][i].values\n",
    "    m = ax0.imshow(tmp[roi_s], vmin=allmin, vmax=allmax)\n",
    "    ax0.set_title(f\"%s:  %s = %s\" % (scan_id, scan_axis, data_bin[scan_axis].values[i]))\n",
    "    plt.colorbar(m, ax=ax0, pad=0.045, location=\"bottom\")\n",
    "\n",
    "    ax1 = fig.add_subplot(gs1[1])\n",
    "    ax1.plot(data_bin[scan_axis].values, data_bin[\"saxs\"].values)\n",
    "    ax1.scatter(\n",
    "        data_bin[scan_axis].values[i], data_bin[\"saxs\"].values[i], 20, color=\"r\"\n",
    "    )\n",
    "    ax1.set_xlabel(scan_axis)\n",
    "    ax1.set_ylabel(\"Integrated intensity\")\n",
    "    ax1.grid()\n",
    "\n",
    "    # Save\n",
    "    fname = join(folder_gif, \"Hysteresis_%s_%03d_%s.png\" % (scan, i, USER))\n",
    "    im_fnames.append(fname)\n",
    "    plt.savefig(fname)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Create gif for 1d AI\n",
    "var = [imageio.imread(file) for file in im_fnames]\n",
    "fname = f\"SAXS_%s_%s.gif\" % (scan, USER)\n",
    "gif_path = join(fsave, fname)\n",
    "print(\"Saving gif:%s\" % gif_path)\n",
    "imageio.mimsave(gif_path, var, fps=2)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop images\n",
    "data_bin = data_bin.drop_vars([\"images\"])\n",
    "\n",
    "# Save log\n",
    "folder = join(fsave, \"Logs\")\n",
    "create_folder(folder)\n",
    "fname = join(folder, \"Log_Hysteresis_Scan_%03d_%s.nc\" % (scan_id, USER))\n",
    "\n",
    "print(f\"Saving:\", fname)\n",
    "data_bin.to_netcdf(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(\n",
    "    samplefolder,\n",
    "    scan,\n",
    "    mask,\n",
    "    ai,\n",
    "    scan_axis,\n",
    "    binning=None,\n",
    "    keys=None,\n",
    "    sort=False,\n",
    "):\n",
    "    # Load scan data\n",
    "    datafolder = join(basefolder, samplefolder)\n",
    "    exp = preprocess_exp_OF(datafolder, keys=keys, sort=sort)\n",
    "\n",
    "    # Load background images\n",
    "    exp_bg = preprocess_exp_BG(datafolder, keys=keys)\n",
    "\n",
    "    # Normalize images\n",
    "    dark = np.mean(np.stack(exp_bg[\"images\"]), axis=0)\n",
    "    images = np.stack(exp.images) - dark\n",
    "    images = images / np.broadcast_to(np.array(exp[\"diode_sum_mean\"]), images.T.shape).T\n",
    "\n",
    "    # Create xarray dataset\n",
    "    data = xr.Dataset()\n",
    "    data[\"images\"] = xr.DataArray(images, dims=[\"index\", \"y\", \"x\"])\n",
    "    data[scan_axis] = xr.DataArray(exp[scan_axis], dims=[\"index\"])\n",
    "\n",
    "    # Do 2d Azimuthal integration of all images and append them to list\n",
    "    list_i2d = []\n",
    "    for im in tqdm(data[\"images\"].values):\n",
    "        i2d, q, chi = ai.integrate2d(im, 500, 90, dummy=np.nan, mask=mask)\n",
    "        list_i2d.append(i2d)\n",
    "\n",
    "    # Add to xarray\n",
    "    data[\"q\"] = q\n",
    "    data[\"chi\"] = chi\n",
    "    data[\"i2d\"] = xr.DataArray(list_i2d, dims=[\"index\", \"chi\", \"q\"])\n",
    "    data[\"i1d\"] = data.i2d.mean(\"chi\")\n",
    "\n",
    "    # Averaging of same scan axis values or binning\n",
    "    if binning is None:\n",
    "        _, count = np.unique(data[scan_axis].values, return_counts=True)\n",
    "        if np.any(count > 1):\n",
    "            data_bin = data.groupby(scan_axis).mean()\n",
    "    else:\n",
    "        # Execute binning\n",
    "        data_bin = data.groupby_bins(scan_axis, bins).mean()\n",
    "\n",
    "        # Rename binned values, drop intervals as those cannot be save in h5\n",
    "        bin_scan_axis = scan_axis + \"_bins\"\n",
    "        data_bin = data_bin.swap_dims({bin_scan_axis: scan_axis})\n",
    "        data_bin = data_bin.drop(bin_scan_axis)\n",
    "\n",
    "    # Add AI mask\n",
    "    data_bin[\"mask\"] = xr.DataArray(mask, dims=[\"y\", \"x\"])\n",
    "\n",
    "    # Add file scan labels\n",
    "    data_bin[\"scan\"] = scan\n",
    "\n",
    "    # Direction of \"time\"\n",
    "    if np.sum(data[scan_axis][1:].values - data[scan_axis][0:-1].values) >= 0:\n",
    "        data_bin[\"order\"] = 1\n",
    "    elif np.sum(data[scan_axis][1:].values - data[scan_axis][0:-1].values) < 0:\n",
    "        data_bin[\"order\"] = -1\n",
    "\n",
    "    # Drop images to save disk space\n",
    "    data_save = data_bin.drop_vars([\"images\"])\n",
    "\n",
    "    # Save log\n",
    "    folder = join(fsave, \"Logs\")\n",
    "    fname = join(folder, \"Log_SAXS_%s_%s.nc\" % (scan, USER))\n",
    "    print(f\"Saving:\", fname)\n",
    "    data_save.to_netcdf(fname)\n",
    "\n",
    "    return data_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of scans (see pad)\n",
    "scans = [\n",
    "    \"I0_HystScan_007\",\n",
    "    \"I0_HystScan_Inverted_008\",\n",
    "    \"I0_HystScan_Inverted_011\",\n",
    "    \"I0_HystScan_Inverted_012\",\n",
    "]\n",
    "\n",
    "# Analysis options\n",
    "bins = []\n",
    "\n",
    "# Setup xarray for scans\n",
    "data_scans = []\n",
    "\n",
    "# Loop over scans\n",
    "for scan in tqdm(scans):\n",
    "    # Process data\n",
    "    samplefolder = join(sample, scan)\n",
    "    data = worker(\n",
    "        samplefolder,\n",
    "        scan,\n",
    "        mask,\n",
    "        ai,\n",
    "        scan_axis,\n",
    "        binning=None,\n",
    "        keys=extra_keys,\n",
    "        sort=False,\n",
    "    )\n",
    "\n",
    "    # Add to xarray list\n",
    "    data_scans.append(data)\n",
    "\n",
    "# Combine separate xarrays\n",
    "data_scans = xr.concat(data_scans, dim=\"scanid\")\n",
    "data_scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calc and plot SAXS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you want to norm the hysteresis?\n",
    "normalization = True\n",
    "\n",
    "# Select relevant q-range for averaging\n",
    "# q0, q1 = 0.003, 0.04\n",
    "\n",
    "# Get SAXS from q-range\n",
    "sel = (data_scans.q > q0) * (data_scans.q < q1)\n",
    "data_scans[\"saxs\"] = data_scans.i1d.where(sel, drop=True).mean(\"q\")\n",
    "\n",
    "# Plot all SAXS images individually\n",
    "for scanid in data_scans[\"scanid\"].values:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Normalize?\n",
    "    if normalization is True:\n",
    "        tmp_data = norm(data_scans[\"saxs\"][scanid].values)\n",
    "    else:\n",
    "        tmp_data = data_scans[\"saxs\"][scanid].values\n",
    "\n",
    "    ax.plot(data_scans[scan_axis].values, tmp_data, \"o-\")\n",
    "    ax.set_xlabel(scan_axis)\n",
    "    ax.set_ylabel(\"Integrated SAXS\")\n",
    "    ax.set_title(\"Scan: %s\" % data_scans[\"scan\"][scanid].values)\n",
    "    ax.grid()\n",
    "\n",
    "    # Save fig\n",
    "    fname = join(fsave, \"SAXS_%s_%s.png\" % (data_scans[\"scan\"][scanid].values, USER))\n",
    "    print(\"Saving: %s\" % fname)\n",
    "    plt.savefig(fname)\n",
    "    plt.close()\n",
    "\n",
    "# Plot them together\n",
    "fig, ax = plt.subplots()\n",
    "for scanid in data_scans[\"scanid\"].values:\n",
    "    # Normalize?\n",
    "    if normalization is True:\n",
    "        tmp_data = norm(data_scans[\"saxs\"][scanid].values)\n",
    "    else:\n",
    "        tmp_data = data_scans[\"saxs\"][scanid].values\n",
    "\n",
    "    ax.plot(\n",
    "        data_scans[scan_axis].values,\n",
    "        tmp_data,\n",
    "        \"o-\",\n",
    "        label=data_scans[\"scan\"][scanid].values,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(scan_axis)\n",
    "ax.set_ylabel(\"Integrated SAXS\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "# Save fig\n",
    "fname = join(\n",
    "    fsave,\n",
    "    \"SAXS_%s_%s_%s.png\"\n",
    "    % (data_scans[\"scan\"][0].values, data_scans[\"scan\"][-1].values, USER),\n",
    ")\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export gif for all scans individually\n",
    "for scan_id in data_scans[\"scanid\"].values:\n",
    "    if data_scans[\"order\"][scan_id] == 1:\n",
    "        tmp_xr = data_scans.copy()\n",
    "    elif data_scans[\"order\"][scan_id] == -1:\n",
    "        tmp_xr = data_scans.reindex(magnet_mean=data_scans.magnet_mean[::-1])\n",
    "\n",
    "    # Find max and min considering all images\n",
    "    allmin, allmax = np.nanpercentile(tmp_xr[\"i2d\"][scan_id].values, [3, 97])\n",
    "    print(\"Min: %d Max: %d\" % (allmin, allmax))\n",
    "\n",
    "    # Create folder for gif single frames\n",
    "    folder_gif = create_folder(join(fsave, \"Scan_%s\" % tmp_xr[\"scan\"][scan_id].values))\n",
    "\n",
    "    # Normalize?\n",
    "    if normalization is True:\n",
    "        tmp_data = norm(tmp_xr[\"saxs\"][scan_id].values)\n",
    "    else:\n",
    "        tmp_data = tmp_xr[\"saxs\"][scan_id].values\n",
    "\n",
    "    im_fnames = []\n",
    "    for i in tqdm(range(len(tmp_xr[scan_axis].values))):\n",
    "        # Plot for averaged image\n",
    "        fig = plt.figure(figsize=(6.5, 10))\n",
    "        gs1 = gridspec.GridSpec(\n",
    "            2,\n",
    "            1,\n",
    "            figure=fig,\n",
    "            left=0.2,\n",
    "            bottom=0.05,\n",
    "            right=0.975,\n",
    "            top=1.1,\n",
    "            wspace=0,\n",
    "            hspace=0,\n",
    "            height_ratios=[3, 1],\n",
    "        )\n",
    "\n",
    "        ax0 = fig.add_subplot(gs1[0])\n",
    "        tmp = tmp_xr[\"images\"][scan_id][i].values\n",
    "        m = ax0.imshow(tmp[roi_s], vmin=allmin, vmax=allmax)\n",
    "        ax0.set_title(\n",
    "            f\"%s:  %s = %s\"\n",
    "            % (tmp_xr[\"scan\"][scanid].values, scan_axis, tmp_xr[scan_axis].values[i])\n",
    "        )\n",
    "        plt.colorbar(m, ax=ax0, pad=0.045, location=\"bottom\")\n",
    "\n",
    "        ax1 = fig.add_subplot(gs1[1])\n",
    "        ax1.plot(tmp_xr[scan_axis].values, tmp_data)\n",
    "        ax1.scatter(\n",
    "            tmp_xr[scan_axis].values[i],\n",
    "            tmp_data[i],\n",
    "            20,\n",
    "            color=\"r\",\n",
    "        )\n",
    "        ax1.set_xlabel(scan_axis)\n",
    "        ax1.set_ylabel(\"Integrated intensity\")\n",
    "        ax1.grid()\n",
    "\n",
    "        # Save\n",
    "        fname = join(\n",
    "            folder_gif,\n",
    "            \"SAXS_%s_%03d_%s.png\" % (tmp_xr[\"scan\"][scan_id].values, i, USER),\n",
    "        )\n",
    "        im_fnames.append(fname)\n",
    "        plt.savefig(fname)\n",
    "        plt.close()\n",
    "\n",
    "    # Create gif for 1d AI\n",
    "    var = [imageio.imread(file) for file in im_fnames]\n",
    "    fname = f\"SAXS_%s_%s.gif\" % (tmp_xr[\"scan\"][scan_id].values, USER)\n",
    "    gif_path = join(fsave, fname)\n",
    "    print(\"Saving gif:%s\" % gif_path)\n",
    "    imageio.mimsave(gif_path, var, fps=2)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "179.1px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 493.85,
   "position": {
    "height": "40px",
    "left": "1580px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
